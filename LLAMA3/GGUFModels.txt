8B: https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/tree/main
13B: https://huggingface.co/TheBloke/Llama-2-13B-GGUF/tree/main
70BQ4: https://huggingface.co/MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF/tree/main OR https://huggingface.co/NousResearch/Meta-Llama-3-70B-GGUF/tree/main (?)

Use llama.cpp:
1. git clone https://github.com/ggerganov/llama.cpp.git
2. Navigate to directory and enter:
            make clean && make LLAMA_HIPBLAS=1
3. Download quanitzed GGUF model from links found above and place them in the model folder nested within llama.cpp
4. Use llama bench to test inference speed of model:
            ./llama-bench --model models/<your_GGUF_model>
